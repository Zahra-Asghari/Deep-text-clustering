{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hizZeKIgrxSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from cleantext import clean\n",
        "import hazm\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Data.xlsx')\n",
        "data = df[~(df['descpishnahad'].duplicated(keep = False))]\n",
        "Data = data[pd.notnull(data['descpishnahad'])]\n",
        "Data.head()\n",
        "\n",
        " df = pd.DataFrame(Data['descpishnahad'])\n",
        " df\n",
        "df['descpishnahad_len_by_words'] = df['descpishnahad'].astype(str).apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "df = df.dropna(subset=['descpishnahad_len_by_words'])\n",
        "df = df.reset_index(drop=True)\n",
        "import re\n",
        "def cleaning(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # regular cleaning\n",
        "    text = clean(text,\n",
        "        fix_unicode=True,\n",
        "        to_ascii=False,\n",
        "        lower=True,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_digits=False,\n",
        "        no_currency_symbols=True,\n",
        "        no_punct=False,\n",
        "        replace_with_url=\"\",\n",
        "        replace_with_email=\"\",\n",
        "        replace_with_phone_number=\"\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "    )\n",
        "\n",
        "\n",
        "    # normalizing\n",
        "    normalizer = hazm.Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "\n",
        "    # removing wierd patterns\n",
        "    wierd_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2069\"\n",
        "        u\"\\u2066\"\n",
        "        u\"\\u200c\"\n",
        "        u\"\\u2068\"\n",
        "        u\"\\u2067\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = wierd_pattern.sub(r'', text)\n",
        "\n",
        "    # removing extra spaces, hashtags\n",
        "    text = re.sub(\"#\", \"\", text)\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text\n",
        "df['cleaned_descpishnahad'] = df['descpishnahad'].astype(str).apply(cleaning)\n",
        "\n",
        "\n",
        "# calculate the length of descpishnahad based on their words\n",
        "df['cleaned_descpishnahad_len_by_words'] = df['cleaned_descpishnahad'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "Df = pd.DataFrame(df['cleaned_descpishnahad'])\n",
        "Df.columns = ['descpishnahad']\n",
        "Df.head()\n",
        "\n",
        "stop_words = hazm.stopwords_list(stopwords_file='/usr/local/lib/python3.6/dist-packages/hazm/data/stopwords.dat')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "   tokens = []\n",
        "   for token in text.split():\n",
        "       if token not in stop_words:\n",
        "          tokens.append(token)\n",
        "   return \" \".join(tokens)\n",
        "\n",
        "Df['cleaned_descpishnahad'] = Df['descpishnahad'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "Df['cleaned_descpishnahad'] = Df['cleaned_descpishnahad'].apply(cleaning)\n",
        "from sentence_transformers import models, SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "corpus = list(Df['cleaned_descpishnahad'])\n",
        "def load_st_model(model_name_or_path):\n",
        "    word_embedding_model = models.Transformer(model_name_or_path)\n",
        "    pooling_model = models.Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False)\n",
        "\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Load the Sentence-Transformer\n",
        "embedder = load_st_model('m3hrdadfi/bert-fa-base-uncased-farstail-mean-tokens')\n",
        "corpus_embeddings = embedder.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "m = Sequential()\n",
        "m.add(Dense(512,  activation='relu', input_shape=(768,)))\n",
        "m.add(Dense(128,  activation='relu'))\n",
        "m.add(Dense(64,  activation='relu'))\n",
        "m.add(Dense(32,  activation='relu'))\n",
        "m.add(Dense(2,    activation='linear', name=\"bottleneck\"))\n",
        "m.add(Dense(32,  activation='relu'))\n",
        "m.add(Dense(64,  activation='relu'))\n",
        "m.add(Dense(128,  activation='relu'))\n",
        "m.add(Dense(512,  activation='relu'))\n",
        "m.add(Dense(768,  activation='sigmoid'))\n",
        "m.compile(loss='mean_squared_error', optimizer = Adam())\n",
        "history = m.fit(corpus_embeddings, corpus_embeddings, batch_size=128, epochs=10, verbose=1)\n",
        "\n",
        "encoder = Model(m.input, m.get_layer('bottleneck').output)\n",
        "Zenc = encoder.predict(corpus_embeddings)\n",
        "Renc = m.predict(corpus_embeddings)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, n_init=50 , random_state=1)\n",
        "kmeans.fit(Renc)\n",
        "score = silhouette_score(Renc, km.labels_, metric='euclidean')\n",
        "score\n"
      ]
    }
  ]
}