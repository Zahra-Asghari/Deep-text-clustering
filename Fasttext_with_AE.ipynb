{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyirEO4mqZgE"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp2SJjiXqZgM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import persian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID3y8t8tqZgN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(r'pishnahdkamel.XLS',usecols=\"G\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMkpre33qZgO"
      },
      "outputs": [],
      "source": [
        "df = df[pd.notnull(df['sharhpishnahad'])]\n",
        "df = df[df.sharhpishnahad.duplicated()==False]\n",
        "df.reset_index(inplace = True)\n",
        "del df['index']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_8Ljpw3qZgQ"
      },
      "outputs": [],
      "source": [
        "stopwords = []\n",
        "file = open('stopwords.txt',encoding = 'utf-8').read()\n",
        "[stopwords.append(x) for x in file.split()]\n",
        "stopwords = set(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JqweZpDqZgR"
      },
      "outputs": [],
      "source": [
        "def removing_stopwords(text):\n",
        "    text =str(text)\n",
        "    filtered_tokens = [token for token in text.split() if token not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s40JJSzhqZgS"
      },
      "outputs": [],
      "source": [
        "def convert_char(text):\n",
        "    text =str(text)\n",
        "    s = ''\n",
        "    for word in text:\n",
        "        s = s + persian.convert_ar_characters(word)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXyb57hHqZgT"
      },
      "outputs": [],
      "source": [
        "def sent_to_word(data):\n",
        "    data = str(data)\n",
        "    words = []\n",
        "    for x in data.split():\n",
        "        words.append(x)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4nSPb8lqZgU"
      },
      "outputs": [],
      "source": [
        "def sent_vectorizer(sent):\n",
        "    sent_vec =[]\n",
        "    numw = 0\n",
        "    for w in sent:\n",
        "        try:\n",
        "            if numw == 0:\n",
        "                sent_vec = ft.get_word_vector(w)\n",
        "            else:\n",
        "                sent_vec = np.add(sent_vec, ft.get_word_vector(w))\n",
        "            numw += 1\n",
        "        except:\n",
        "            pass\n",
        "    return np.asarray(sent_vec) / numw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg_D8DgfqZgW"
      },
      "outputs": [],
      "source": [
        "sent_words = []\n",
        "for i in df.sharhpishnahad.index:\n",
        "    temp = removing_stopwords(df.sharhpishnahad[i])\n",
        "    temp = convert_char(temp)\n",
        "    temp = removing_stopwords(temp)\n",
        "    temp = sent_to_word(temp)\n",
        "    sent_words.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niroheR0qZgX"
      },
      "outputs": [],
      "source": [
        "vec = []\n",
        "for sentence in sent_words:\n",
        "    vec.append(sent_vectorizer(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-WiHqeLqZgY"
      },
      "outputs": [],
      "source": [
        "del_row_id = []\n",
        "del_row_value = []\n",
        "for i in range(len(vec)):\n",
        "    if len(vec[i]) == 0:\n",
        "        del_row_id.append(i)\n",
        "        tt = vec[i]\n",
        "        del_row_value.append(tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iXaQBJcqZga"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df.index [del_row_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmTu6KcQqZga"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop = True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkOxh-zQqZgb"
      },
      "outputs": [],
      "source": [
        "new_vec = []\n",
        "for e in vec:\n",
        "    if e not in del_row_value:\n",
        "        new_vec.append(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TVxtcX8qZgc"
      },
      "outputs": [],
      "source": [
        "vec_array = np.vstack(new_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcvV4XcUqZgc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import load_model as load_keras_model\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSDZD0WxqZgd"
      },
      "outputs": [],
      "source": [
        "input_vec = Input(shape=(ENCODING_DIM_INPUT,))\n",
        "\n",
        "encoded = Dense(ENCODING_DIM_OUTPUT_1)(input_vec)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = LeakyReLU()(encoded)\n",
        "\n",
        "encoded = Dense(ENCODING_DIM_OUTPUT_2)(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = LeakyReLU()(encoded)\n",
        "\n",
        "encoded = Dense(ENCODING_DIM_OUTPUT_3)(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = LeakyReLU()(encoded)\n",
        "\n",
        "decoded = Dense(ENCODING_DIM_OUTPUT_2)(encoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = LeakyReLU()(decoded)\n",
        "\n",
        "decoded = Dense(ENCODING_DIM_OUTPUT_1)(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = LeakyReLU()(decoded)\n",
        "\n",
        "decoded = Dense(ENCODING_DIM_INPUT)(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = LeakyReLU()(decoded)\n",
        "\n",
        "autoencoder = Model(inputs = input_vec, outputs = decoded)\n",
        "encoder = Model(inputs = input_vec, outputs = encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n84VjphqZge"
      },
      "outputs": [],
      "source": [
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m13tU20zqZgf"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(vec_array, vec_array, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9xeEsi0qZgg"
      },
      "outputs": [],
      "source": [
        "encoded_vec = encoder.predict(vec_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QQ9SAngqZgh"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVstGSCpqZgi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sse = {}\n",
        "for k in range(2, 10):\n",
        "    kmeans_s = KMeans(n_clusters=k, max_iter=100).fit(encoded_vec)\n",
        "    sse[k] = kmeans_s.inertia_\n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xkipyfpqZgi"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters = 3, n_init = 50, random_state=1)\n",
        "kmeans.fit(encoded_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9CfmgOqZgj"
      },
      "outputs": [],
      "source": [
        "labels = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YoP3k2IqZgk"
      },
      "outputs": [],
      "source": [
        "df.loc[:,'labels'] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1onLiv5qZgk"
      },
      "outputs": [],
      "source": [
        "df.labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjKFKQ3-qZgk"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "metrics.silhouette_score(encoded_vec, labels, metric='euclidean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Njbb7oJqZgl"
      },
      "outputs": [],
      "source": [
        "df.to_excel('Barez_1_plus_AE.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}