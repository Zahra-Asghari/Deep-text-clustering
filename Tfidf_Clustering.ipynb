{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpedeT1lvwLx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import persian\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "df = pd.read_excel(r'...\\pishnahdkamel.XLS' )\n",
        "df = df[pd.notnull(df['sharhpishnahad'])]  #حذف سطرهایی که ستون پیشنهادات آن خالی است\n",
        "temp = df[df.sharhpishnahad.duplicated()==False]#حذف پیشنهادهای تکراری\n",
        "\n",
        "#کل اطلاعات ستون پیشنهادات را درکنارهم نمایش میدهد\n",
        "print(temp.loc[0, 'sharhpishnahad'])\n",
        "\n",
        "temp[\"original\"] = temp.sharhpishnahad\n",
        "\n",
        "stopwords = []\n",
        "file = open('stopwords.txt', encoding = 'utf-8').read()\n",
        "[stopwords.append(x) for x in file.split()]\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "#برای حذف لغات زاید بکار میرود درواقع جایگذاری باخالی میشود\n",
        "def remove_general_stopwords(text):\n",
        "    text = str(text)\n",
        "    filtered_tokens = [token for token in text.split() if token not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "#برای تبدیل حروف ی و ک بکار میرود\n",
        "def convert_ar_chars(text):\n",
        "    s = ''\n",
        "    for word in text:\n",
        "        s = s + persian.convert_ar_characters(word)\n",
        "    return s\n",
        "\n",
        "temp.sharhpishnahad = temp.sharhpishnahad.apply(remove_general_stopwords)\n",
        "\n",
        "temp.sharhpishnahad= temp.sharhpishnahad.apply(convert_ar_chars)\n",
        "temp.sharhpishnahad = temp.sharhpishnahad.apply(remove_general_stopwords)\n",
        "\n",
        "#تعداد تکرار هرکلمه در ستون پیشنهاد را نشان میدهد\n",
        "w_count = temp.sharhpishnahad.str.split(expand=True).stack().value_counts()\n",
        "\n",
        "\n",
        "X = temp.sharhpishnahad\n",
        "\n",
        "#کلماتی که حداقل تعدادتکرارآنها 100 است را باروش tfidf تبدیل به یک آرایه کن\n",
        "# هریک از لغات درواقع یک هنصرآرایه است وبه مدل نگاشته میشود\n",
        "model = Pipeline([('vect', CountVectorizer(min_df=100)),\n",
        "                  ('tfidf', TfidfTransformer(sublinear_tf=True))])\n",
        "XX = model.fit_transform(X)\n",
        "\n",
        "#چون هریک از لغات به عنوان یک ویژگی است\n",
        "model.named_steps['vect'].get_feature_names()\n",
        "\n",
        "#از دیتای ساخته شده برای مدل یک دیتافریم ساخته میشود\n",
        "new_df = pd.DataFrame(XX.toarray(),\n",
        "                        columns = model.named_steps['vect'].get_feature_names())\n",
        "\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters = 3, random_state=1)\n",
        "kmeans.fit(new_df)\n",
        "\n",
        "# temp['labels'] = kmeans.labels_\n",
        "temp.loc[:,'labels'] = kmeans.labels_\n",
        "\n",
        "temp.labels.value_counts()\n",
        "\n",
        "\n",
        "sse = {}\n",
        "for k in range(2, 10):\n",
        "    kmeans_s = KMeans(n_clusters=k, max_iter=100).fit(new_df)\n",
        "    sse[k] = kmeans_s.inertia_\n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#برای ذخیره مدل استفاده میشود که برای دفعات بعدی مدل لود شود ویک متن به عنوان ورودی داده شود وبراساس مدل فیت شده قبلی بتواند جواب را استخراج کند\n",
        "with open('kmeans.pickle','wb') as f:\n",
        "    pickle.dump(kmeans, f)\n",
        "with open('kmeans.pickle','rb') as f:\n",
        "    clf = pickle.load(f)\n",
        "\n",
        "sample = \"\"\"\n",
        "نصب دستگاه کالیبراسیون درخط تولیدبرای اندازه گیری آنلاین\n",
        "\"\"\"\n",
        "\n",
        "sample = model.transform([sample]).toarray()\n",
        "sentiment = clf.predict(sample)"
      ]
    }
  ]
}